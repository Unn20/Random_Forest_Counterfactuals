{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ec08304",
   "metadata": {},
   "source": [
    "Experiment shows differences in use different costs functions on wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1b211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from rf_counterfactuals import RandomForestExplainer, visualize, evaluate_counterfactual, evaluate_counterfactual_set\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "DATASET_PATH = \"./datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0add4e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(os.path.join(DATASET_PATH, \"wine_data.csv\"), sep=',')\n",
    "\n",
    "dataset.columns = ['quality', 'alcohol', 'malic acid', 'ash', 'alcalinity of ash', 'magnesium', 'total phenols', 'flavanoids',\n",
    "                  'nonflavanoid phenols', 'proanthocyanis', 'color intensity', 'hue', 'OD280/OD315 of diluted wines', 'proline'\n",
    "                  ]\n",
    "\n",
    "class_feature = \"quality\"\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(['1', '2', '3'], dataset[class_feature].value_counts()[dataset[class_feature].unique()])\n",
    "fig.gca().bar_label(bars)\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Wine dataset. Class labels distribution\")\n",
    "plt.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139a03db",
   "metadata": {},
   "source": [
    "# 30 trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c11baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = iter([3, 5])\n",
    "N_ESTIMATORS = 30\n",
    "\n",
    "METRICS = []\n",
    "METRICS += ['euclidean', 'cosine', 'jaccard', 'pearson_correlation', 'unmatched_components', 'hoem', 'implausibility_single']\n",
    "METRICS += ['k_nearest_neighborhood', 'k_nearest_neighborhood']\n",
    "\n",
    "RETRY = 1\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1000)\n",
    "X = dataset.loc[:, dataset.columns!=class_feature]\n",
    "y = dataset[class_feature]\n",
    "\n",
    "cfs2_dict = {}\n",
    "cfs3_dict = {}\n",
    "times2 = {}\n",
    "times3 = {}\n",
    "times23 = {}\n",
    "\n",
    "scores2 = []\n",
    "scores3 = []\n",
    "scores23 = []\n",
    "\n",
    "accuraces30 = []\n",
    "\n",
    "START = time.time()\n",
    "\n",
    "for metric in METRICS:\n",
    "    cfs2_dict[metric] = []\n",
    "    cfs3_dict[metric] = []\n",
    "    times2[metric] = []\n",
    "    times3[metric] = []\n",
    "    times23[metric] = []\n",
    "    \n",
    "    s2 = []\n",
    "    s3 = []\n",
    "    s23 = []\n",
    "    \n",
    "    k = 5\n",
    "    if \"nearest_neighborhood\" in metric:\n",
    "        k = next(K)\n",
    "\n",
    "    print(metric, k)\n",
    "\n",
    "\n",
    "    split = 0\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        print(f\"split {split+1}/5\")\n",
    "        split += 1\n",
    "        print(y_test.value_counts())\n",
    "        \n",
    "        X_test_label_1 = X_test[y_test==1]\n",
    "        X_test_label_2 = X_test[y_test==2]\n",
    "\n",
    "        for i in range(RETRY):\n",
    "            print(f\"{i+1}/{RETRY}\")\n",
    "            \n",
    "            rf = RandomForestClassifier(n_estimators=N_ESTIMATORS, max_depth=None, random_state=i*10+1000)\n",
    "            rf.fit(X_train, y_train)\n",
    "            acc_score = accuracy_score(rf.predict(X_test), y_test)\n",
    "            accuraces30.append(acc_score)\n",
    "#             print(acc_score)\n",
    "\n",
    "            rfe = RandomForestExplainer(rf, X_train, y_train)\n",
    "            start_time = time.time()\n",
    "            cfs_2 = rfe.explain_with_single_metric(X_test_label_1, 2, k=k, metric=metric, limit=1)\n",
    "            end_time = time.time()\n",
    "\n",
    "            times2[metric] += [end_time - start_time]\n",
    "            print(f\"Total counterfactuals found: {sum([len(c) for c in cfs_2])}\")\n",
    "            print(f\"Finished in {end_time - start_time: 1.4f}s\")\n",
    "            \n",
    "            for i in range(len(cfs_2)):\n",
    "                if len(cfs_2[i]) == 0:\n",
    "                    continue\n",
    "                e = evaluate_counterfactual(rfe, X_test_label_1.iloc[i], cfs_2[i].iloc[0], 5)\n",
    "                s2.append([e['sparsity'], e['proximity'], e['implausibility']])\n",
    "            \n",
    "\n",
    "            rfe = RandomForestExplainer(rf, X_train, y_train)\n",
    "            start_time = time.time()\n",
    "            cfs_3 = rfe.explain_with_single_metric(X_test_label_1, 3, k=k, metric=metric, limit=1)\n",
    "            end_time = time.time()\n",
    "\n",
    "            times3[metric] += [end_time - start_time]\n",
    "            print(f\"Total counterfactuals found: {sum([len(c) for c in cfs_3])}\")\n",
    "            print(f\"Finished in {end_time - start_time: 1.4f}s\")\n",
    "            \n",
    "            for i in range(len(cfs_3)):\n",
    "                if len(cfs_3[i]) == 0:\n",
    "                    continue\n",
    "                e = evaluate_counterfactual(rfe, X_test_label_1.iloc[i], cfs_3[i].iloc[0], 5)\n",
    "                s3.append([e['sparsity'], e['proximity'], e['implausibility']])\n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "    means, stds = np.mean(np.array(s2), axis=0), np.std(np.array(s2), axis=0)\n",
    "    scores2.append([means, stds])\n",
    "\n",
    "    means, stds = np.mean(np.array(s3), axis=0), np.std(np.array(s3), axis=0)\n",
    "    scores3.append([means, stds])\n",
    "    \n",
    "#     means, stds = np.mean(np.array(s23), axis=0), np.std(np.array(s23), axis=0)\n",
    "#     scores23.append([means, stds])\n",
    "    \n",
    "    means, stds = np.mean(times2[metric]), np.std(times2[metric])\n",
    "    times2[metric] = [means, stds]\n",
    "    means, stds = np.mean(times3[metric]), np.std(times3[metric])\n",
    "    times3[metric] = [means, stds]\n",
    "#     means, stds = np.mean(times23[metric]), np.std(times23[metric])\n",
    "#     times23[metric] = [means, stds]\n",
    "\n",
    "    \n",
    "    \n",
    "END = time.time()\n",
    "\n",
    "print(f\"TOTAL TIME = {END - START}s\")\n",
    "    \n",
    "scores2 = np.array(scores2)\n",
    "scores3 = np.array(scores3)\n",
    "scores23 = np.array(scores23)\n",
    "scores3.shape \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd30d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = iter([3, 5])\n",
    "\n",
    "for no, metric in enumerate(METRICS):\n",
    "    m = metric.replace(\"_\", \"\\\\_\")\n",
    "    if 'nearest' in m:\n",
    "        m = f\"{next(K)}\" + m[1:]\n",
    "    print(f\"{m} & {scores2[no, 0, 0]:1.3f}({scores2[no, 1, 0]:1.3f}) & {scores2[no, 0, 1]:1.3f}({scores2[no, 1, 1]:1.3f}) & {scores2[no, 0, 2]:1.3f}({scores2[no, 1, 2]:1.3f}) \\\\\\\\\")\n",
    "    \n",
    "for no, metric in enumerate(METRICS):\n",
    "    print(f\"{times2[metric]}\")\n",
    "    \n",
    "    \n",
    "K = iter([3, 5])\n",
    "for no, metric in enumerate(METRICS):\n",
    "    m = metric.replace(\"_\", \"\\\\_\")\n",
    "    if 'nearest' in m:\n",
    "        m = f\"{next(K)}\" + m[1:]\n",
    "    print(f\"{m} & {scores3[no, 0, 0]:1.3f}({scores3[no, 1, 0]:1.3f}) & {scores3[no, 0, 1]:1.3f}({scores3[no, 1, 1]:1.3f}) & {scores3[no, 0, 2]:1.3f}({scores3[no, 1, 2]:1.3f}) \\\\\\\\\")\n",
    "    \n",
    "for no, metric in enumerate(METRICS):\n",
    "    print(f\"{times3[metric]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9996560",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(10, 16))\n",
    "\n",
    "width=0.3\n",
    "y_pos = np.arange(len(METRICS))\n",
    "\n",
    "for no, met in enumerate(['sparsity', 'proximity', 'implausibility']):\n",
    "    label = met\n",
    "    if 'implausibility' in label:\n",
    "        label += '(k=5)'\n",
    "    ax[0].barh(y_pos + (no-1)*width, scores2[:, 0, no], width, xerr=scores2[:, 1, no], align='center', label=label)\n",
    "    ax[1].barh(y_pos + (no-1)*width, scores3[:, 0, no], width, xerr=scores3[:, 1, no], align='center', label=label)\n",
    "\n",
    "\n",
    "    \n",
    "for i in range(2):\n",
    "    ax[i].set_xlim([0.0, 1.5])\n",
    "    metrics = METRICS.copy()\n",
    "    metrics[-3] = metrics[-3] + '(k=5)'\n",
    "    metrics[-2] = '3' + metrics[-2][1:]\n",
    "    metrics[-1] = '5' + metrics[-1][1:]\n",
    "    ax[i].set_yticks(y_pos, labels=metrics)\n",
    "    ax[i].legend()\n",
    "    ax[i].grid()\n",
    "    ax[i].invert_yaxis()  # labels read top-to-bottom\n",
    "    ax[i].set_xlabel('Value')\n",
    "    ax[i].set_ylabel(\"Metric\")\n",
    "    ax[i].set_title(f\"Random forest (n_estimators={N_ESTIMATORS}). 5-fold-cross-validation. Label change: '1' -> '{i+2}'\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0388bcd",
   "metadata": {},
   "source": [
    "# 100 trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c720557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = iter([3, 5])\n",
    "N_ESTIMATORS = 100\n",
    "\n",
    "METRICS = []\n",
    "METRICS += ['euclidean', 'cosine', 'jaccard', 'pearson_correlation', 'unmatched_components', 'hoem', 'implausibility_single']\n",
    "METRICS += ['k_nearest_neighborhood', 'k_nearest_neighborhood']\n",
    "\n",
    "RETRY = 1\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1000)\n",
    "X = dataset.loc[:, dataset.columns!=class_feature]\n",
    "y = dataset[class_feature]\n",
    "\n",
    "cfs2_dict = {}\n",
    "cfs3_dict = {}\n",
    "times2 = {}\n",
    "times3 = {}\n",
    "times23 = {}\n",
    "\n",
    "scores2 = []\n",
    "scores3 = []\n",
    "scores23 = []\n",
    "\n",
    "accuraces100 = []\n",
    "\n",
    "START = time.time()\n",
    "\n",
    "for metric in METRICS:\n",
    "    cfs2_dict[metric] = []\n",
    "    cfs3_dict[metric] = []\n",
    "    times2[metric] = []\n",
    "    times3[metric] = []\n",
    "    times23[metric] = []\n",
    "    \n",
    "    s2 = []\n",
    "    s3 = []\n",
    "    s23 = []\n",
    "    \n",
    "    k = 5\n",
    "    if \"nearest_neighborhood\" in metric:\n",
    "        k = next(K)\n",
    "\n",
    "    print(metric, k)\n",
    "\n",
    "\n",
    "    split = 0\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        print(f\"split {split+1}/5\")\n",
    "        split += 1\n",
    "        print(y_test.value_counts())\n",
    "        \n",
    "        X_test_label_1 = X_test[y_test==1]\n",
    "        X_test_label_2 = X_test[y_test==2]\n",
    "\n",
    "        for i in range(RETRY):\n",
    "            print(f\"{i+1}/{RETRY}\")\n",
    "            \n",
    "            rf = RandomForestClassifier(n_estimators=N_ESTIMATORS, max_depth=None, random_state=i*10+1000)\n",
    "            rf.fit(X_train, y_train)\n",
    "            acc_score = accuracy_score(rf.predict(X_test), y_test)\n",
    "            accuraces100.append(acc_score)\n",
    "#             print(acc_score)\n",
    "\n",
    "            rfe = RandomForestExplainer(rf, X_train, y_train)\n",
    "            start_time = time.time()\n",
    "            cfs_2 = rfe.explain_with_single_metric(X_test_label_1, 2, k=k, metric=metric, limit=1)\n",
    "            end_time = time.time()\n",
    "\n",
    "            times2[metric] += [end_time - start_time]\n",
    "            print(f\"Total counterfactuals found: {sum([len(c) for c in cfs_2])}\")\n",
    "            print(f\"Finished in {end_time - start_time: 1.4f}s\")\n",
    "            \n",
    "            for i in range(len(cfs_2)):\n",
    "                if len(cfs_2[i]) == 0:\n",
    "                    continue\n",
    "                e = evaluate_counterfactual(rfe, X_test_label_1.iloc[i], cfs_2[i].iloc[0], 5)\n",
    "                s2.append([e['sparsity'], e['proximity'], e['implausibility']])\n",
    "            \n",
    "\n",
    "            rfe = RandomForestExplainer(rf, X_train, y_train)\n",
    "            start_time = time.time()\n",
    "            cfs_3 = rfe.explain_with_single_metric(X_test_label_1, 3, k=k, metric=metric, limit=1)\n",
    "            end_time = time.time()\n",
    "\n",
    "            times3[metric] += [end_time - start_time]\n",
    "            print(f\"Total counterfactuals found: {sum([len(c) for c in cfs_3])}\")\n",
    "            print(f\"Finished in {end_time - start_time: 1.4f}s\")\n",
    "            \n",
    "            for i in range(len(cfs_3)):\n",
    "                if len(cfs_3[i]) == 0:\n",
    "                    continue\n",
    "                e = evaluate_counterfactual(rfe, X_test_label_1.iloc[i], cfs_3[i].iloc[0], 5)\n",
    "                s3.append([e['sparsity'], e['proximity'], e['implausibility']])\n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "    means, stds = np.mean(np.array(s2), axis=0), np.std(np.array(s2), axis=0)\n",
    "    scores2.append([means, stds])\n",
    "\n",
    "    means, stds = np.mean(np.array(s3), axis=0), np.std(np.array(s3), axis=0)\n",
    "    scores3.append([means, stds])\n",
    "    \n",
    "#     means, stds = np.mean(np.array(s23), axis=0), np.std(np.array(s23), axis=0)\n",
    "#     scores23.append([means, stds])\n",
    "    \n",
    "    means, stds = np.mean(times2[metric]), np.std(times2[metric])\n",
    "    times2[metric] = [means, stds]\n",
    "    means, stds = np.mean(times3[metric]), np.std(times3[metric])\n",
    "    times3[metric] = [means, stds]\n",
    "#     means, stds = np.mean(times23[metric]), np.std(times23[metric])\n",
    "#     times23[metric] = [means, stds]\n",
    "\n",
    "    \n",
    "    \n",
    "END = time.time()\n",
    "\n",
    "print(f\"TOTAL TIME = {END - START}s\")\n",
    "    \n",
    "scores2 = np.array(scores2)\n",
    "scores3 = np.array(scores3)\n",
    "scores23 = np.array(scores23)\n",
    "scores3.shape \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871c0196",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = iter([3, 5])\n",
    "\n",
    "for no, metric in enumerate(METRICS):\n",
    "    m = metric.replace(\"_\", \"\\\\_\")\n",
    "    if 'nearest' in m:\n",
    "        m = f\"{next(K)}\" + m[1:]\n",
    "    print(f\"{m} & {scores2[no, 0, 0]:1.3f}({scores2[no, 1, 0]:1.3f}) & {scores2[no, 0, 1]:1.3f}({scores2[no, 1, 1]:1.3f}) & {scores2[no, 0, 2]:1.3f}({scores2[no, 1, 2]:1.3f}) \\\\\\\\\")\n",
    "    \n",
    "for no, metric in enumerate(METRICS):\n",
    "    print(f\"{times2[metric]}\")\n",
    "    \n",
    "    \n",
    "K = iter([3, 5])\n",
    "for no, metric in enumerate(METRICS):\n",
    "    m = metric.replace(\"_\", \"\\\\_\")\n",
    "    if 'nearest' in m:\n",
    "        m = f\"{next(K)}\" + m[1:]\n",
    "    print(f\"{m} & {scores3[no, 0, 0]:1.3f}({scores3[no, 1, 0]:1.3f}) & {scores3[no, 0, 1]:1.3f}({scores3[no, 1, 1]:1.3f}) & {scores3[no, 0, 2]:1.3f}({scores3[no, 1, 2]:1.3f}) \\\\\\\\\")\n",
    "    \n",
    "for no, metric in enumerate(METRICS):\n",
    "    print(f\"{times3[metric]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3f0c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(10, 16))\n",
    "\n",
    "width=0.3\n",
    "y_pos = np.arange(len(METRICS))\n",
    "\n",
    "for no, met in enumerate(['sparsity', 'proximity', 'implausibility']):\n",
    "    label = met\n",
    "    if 'implausibility' in label:\n",
    "        label += '(k=5)'\n",
    "    ax[0].barh(y_pos + (no-1)*width, scores2[:, 0, no], width, xerr=scores2[:, 1, no], align='center', label=label)\n",
    "    ax[1].barh(y_pos + (no-1)*width, scores3[:, 0, no], width, xerr=scores3[:, 1, no], align='center', label=label)\n",
    "\n",
    "\n",
    "    \n",
    "for i in range(2):\n",
    "    ax[i].set_xlim([0.0, 1.5])\n",
    "    metrics = METRICS.copy()\n",
    "    metrics[-3] = metrics[-3] + '(k=5)'\n",
    "    metrics[-2] = '3' + metrics[-2][1:]\n",
    "    metrics[-1] = '5' + metrics[-1][1:]\n",
    "    ax[i].set_yticks(y_pos, labels=metrics)\n",
    "    ax[i].legend()\n",
    "    ax[i].grid()\n",
    "    ax[i].invert_yaxis()  # labels read top-to-bottom\n",
    "    ax[i].set_xlabel('Value')\n",
    "    ax[i].set_ylabel(\"Metric\")\n",
    "    ax[i].set_title(f\"Random forest (n_estimators={N_ESTIMATORS}). 5-fold-cross-validation. Label change: '1' -> '{i+2}'\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da92849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.boxplot([accuraces30, accuraces])\n",
    "\n",
    "plt.xticks([1, 2], ['30', '100'])\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Wine dataset. Accuracy distribition on 5-fold-cv test data\")\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rf-counterfactuals-venv",
   "language": "python",
   "name": "rf-counterfactuals-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
